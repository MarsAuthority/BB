import{ad as o,H as p,I as i,F as n,U as t,O as s,ae as e,X as c}from"./framework-0d2eacdd.js";const d={},r=e(`<h2 id="什么是n-gram" tabindex="-1"><a class="header-anchor" href="#什么是n-gram" aria-hidden="true">#</a> 什么是N-Gram？</h2><p>N-Gram是一种在自然语言处理(NLP)中常用的一种概率语言模型(Probabilistic Language Model)，常用于语音\\手写识别、机器翻译、拼写纠错等等领域。</p><p>它的本质就是计算一个句子或者一连串词出现的概率。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token operator">/</span><span class="token operator">*</span>
T 是由 W1<span class="token punctuation">,</span>W2<span class="token punctuation">,</span>W3<span class="token punctuation">,</span>W4<span class="token punctuation">,</span>W5 <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> Wn组成的一个句子。
<span class="token operator">*</span><span class="token operator">/</span>

P<span class="token punctuation">(</span>T<span class="token punctuation">)</span> <span class="token operator">=</span> P<span class="token punctuation">(</span>W1<span class="token punctuation">,</span>W2<span class="token punctuation">,</span>W3<span class="token punctuation">,</span>W4<span class="token punctuation">,</span>W5 <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> Wn<span class="token punctuation">)</span> <span class="token operator">//</span>这个句子出现的概率是里面每一个词出现的概率的叠加。

P<span class="token punctuation">(</span>W5<span class="token operator">|</span>W1<span class="token punctuation">,</span>W2<span class="token punctuation">,</span>W3<span class="token punctuation">,</span>W4<span class="token punctuation">)</span> <span class="token operator">//</span>已经出现第<span class="token number">1</span>个至第<span class="token number">4</span>个的词的情况下，第<span class="token number">5</span>个词出现的概率。
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>比如：</p><figure><img src="https://cdn.jsdelivr.net/gh/MarsAuthority/sec_pic@master/uPic/2023-02/r9iFeU.jpg" alt="Google" tabindex="0" loading="lazy"><figcaption>Google</figcaption></figure>`,6),l=n("code",null,"at",-1),u=n("code",null,"in",-1),h=n("code",null,"for",-1),k=n("code",null,"refrigerator",-1),m=n("code",null,"throw",-1),g=n("code",null,"gull",-1),b={href:"https://en.wikipedia.org/wiki/Chain_rule_(probability)",target:"_blank",rel:"noopener noreferrer"},f=e("<ul><li>2个事件同时发生的概率：<strong>P(a, b) = P(a | b) * P(b)</strong></li><li>3个事件的概率链式调用：<strong>P(a, b, c) = P(a | b, c) * P(b, c) = P(a | b, c) * P(b | c) * P(c)</strong></li><li>推广到N个事件，概率链式法则为：<strong>P(X1, X2, ... Xn) = P(X1 | X2, X3 ... Xn) * P(X2 | X3, X4 ... Xn) ... P(Xn-1 | Xn) * P(Xn)</strong></li></ul><p>但是这样会有两个问题：</p><ol><li>参数空间过大，不可能实用化。（N越大越难计算）</li><li>数据稀疏严重，语言有各种各样的组合，数据量太大，无法获取这么全的数据。</li></ol><p>所以为了简化这个问题，我们引入马尔科夫假设（Markov Assumption）：”一个词的出现仅仅依赖于它前面出现的一个或者有限的几个词。”</p><ol><li>如果一个词的出现仅仅依赖于它本身，我们称之为 Uni-gram model : <code>P(T) = P(W1)P(W2)...P(Wn)</code></li><li>如果一个词的出现仅仅依赖于它前面出现的一个词，我们称之为 Bi-gram model : <code>P(T) = P(W1)P(W2|W1)P(W3|W2)...P(Wn|Wn-1)</code></li><li>如果一个词的出现仅仅依赖于它前面出现的两个词，我们称之为 Tri-gram model : <code>P(T) = P(W1)P(W3|W1,W2)...P(Wn|Wn-2,Wn-1)</code></li><li>依次类推到仅依赖于它前面出现的N个词，还有4-gram, 5-gram。</li></ol>",5),_={href:"http://www1.icsi.berkeley.edu/Speech/berp.html",target:"_blank",rel:"noopener noreferrer"},v=e(`<p>词和词频率：</p><table><thead><tr><th>i</th><th>want</th><th>to</th><th>eat</th><th>chinese</th><th>food</th><th>lunch</th><th>spend</th></tr></thead><tbody><tr><td>2533</td><td>927</td><td>2417</td><td>746</td><td>158</td><td>1093</td><td>341</td><td>278</td></tr></tbody></table><p>词序列频率：</p><table><thead><tr><th>*</th><th>i</th><th>want</th><th>to</th><th>eat</th><th>chinese</th><th>food</th><th>lunch</th><th>spend</th></tr></thead><tbody><tr><td>i</td><td>5</td><td>827</td><td>0</td><td>9</td><td>0</td><td>0</td><td>0</td><td>2</td></tr><tr><td>want</td><td>2</td><td>0</td><td>608</td><td>1</td><td>6</td><td>6</td><td>5</td><td>1</td></tr><tr><td>to</td><td>2</td><td>0</td><td>4</td><td>686</td><td>2</td><td>0</td><td>6</td><td>211</td></tr><tr><td>eat</td><td>0</td><td>0</td><td>2</td><td>0</td><td>16</td><td>2</td><td>42</td><td>0</td></tr><tr><td>chinese</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>82</td><td>1</td><td>0</td></tr><tr><td>food</td><td>15</td><td>0</td><td>15</td><td>0</td><td>1</td><td>4</td><td>0</td><td>0</td></tr><tr><td>lunch</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>spend</td><td>1</td><td>0</td><td>1</td><td>9</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table><p>根据上表我们可以直观的看出，在这八个词的组合中，概率最高的句子是： i want to eat lunch 。它的概率是 P(i want to eat lunch) = P(i)P(want|i)P(to|want)P(eat|to)P(lunch|eat) = 2533/10132 * <em>827/2533</em> * 608/927 * <em>686/2417</em> * 42/746 = 0.25 * <em>0.326</em> * 0.656 * <em>0.284</em> * 0.056 = 0.00085</p><h3 id="smoothing" tabindex="-1"><a class="header-anchor" href="#smoothing" aria-hidden="true">#</a> Smoothing</h3><p>随着N-Grams的N的增大，N-Grams的数量对越来越多。如果词表中有10000个词，Bi-Gram模型可能产生100000000个N-Gram，Tri-Gram模型则可能产生1000000000000个N-Gram，那么会出现(unseen events)，词库中的某些词在训练样本中没有的情况（比如<code>in</code>在训练样本中没有出现在<code>turn</code> 后面）。为了避免在这种情况下概率为0，我们使用Smoothing来解决。</p><ol><li><p>add-1 smoothing : 很简单，给所有统计的counts加 1 。</p></li><li><p>add-k smoothing : 将高概率分到unseen events，在计算概率的时候，选择一个合适的k值。</p><figure><img src="https://cdn.jsdelivr.net/gh/MarsAuthority/sec_pic@master/uPic/2023-02/MEKSo5.jpg" alt="add-k" tabindex="0" loading="lazy"><figcaption>add-k</figcaption></figure></li><li><p>backoff ： 如果Tri-gram统计为0，就去看Bi-gram，以此类推。</p></li><li><p>interpolation : 以权重，将Tri-gram，Bi-gram，Uni-gram综合起来。（举例）</p></li><li><p>kneser-ney smoothing : 以地位向高位，或者高位向地位的方向，传递高频。</p></li></ol><h2 id="使用python生成n-grams" tabindex="-1"><a class="header-anchor" href="#使用python生成n-grams" aria-hidden="true">#</a> 使用Python生成N-grams</h2><p>简单的实现：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">NGram</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
		self<span class="token punctuation">.</span>n <span class="token operator">=</span> n
		self<span class="token punctuation">.</span>table <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
		self<span class="token punctuation">.</span>parse_text<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">,</span> n<span class="token punctuation">)</span>

	<span class="token keyword">def</span> <span class="token function">parse_text</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token keyword">for</span> letter <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span>text<span class="token punctuation">[</span>i<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
			self<span class="token punctuation">.</span>table<span class="token punctuation">[</span><span class="token string">&#39;&#39;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>letter<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>table<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">&#39;&#39;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>letter<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span> <span class="token comment"># increment count</span>

<span class="token keyword">print</span> NGram<span class="token punctuation">(</span><span class="token string">&quot;abcdef&quot;</span><span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>table
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="nids中的应用" tabindex="-1"><a class="header-anchor" href="#nids中的应用" aria-hidden="true">#</a> NIDS中的应用</h2><h3 id="案例一" tabindex="-1"><a class="header-anchor" href="#案例一" aria-hidden="true">#</a> 案例一</h3><p>Ke Wang 和 Salvatore J. Stolfo 在《Anomalous Payload-based Network Intrusion Detection 》中提出了一种基于1-Gram的方法，将数据包以端口分类，相同端口的数据包再以不同的长度分类，然后计算出ASCII字符0-255的平均分布频率，作为一个特征，加上平均分布频率的平均值，方差，标准差作为另一个特征。有了这两个特征，就可以在异常检测中建立模型，完成任务。如下图：</p><figure><img src="https://cdn.jsdelivr.net/gh/MarsAuthority/sec_pic@master/uPic/2023-02/qvrUtF.jpg" alt="案例一" tabindex="0" loading="lazy"><figcaption>案例一</figcaption></figure><p>整个思路是：是训练阶段，算出不同端口/长度数据包的平均字节概率分布模型(平均值，方差，标准差)，预测阶段，算出新数据包的字节概率分布模型，使用马氏距离(Mahalanobis distance)，比较两个模型的差异，当差异超过某个阈值的时候，则检测出异常。还加上了增量学习(Incremental learning)使整个模型随着新数据的到来，不断的更新自己的参数(平均值，方差，标准差)，”淘汰”旧数据，”更新”新数据。</p><p>文中还提到了一种实现签名检测的方法：将字节的平均概率分布图，把频率从高到低进行重排序。这样得出的分布图很像Zipf-like分布(指数函数/幂函数少数值频繁出现，多数值偶尔出现。通俗地讲，就是二八原则：80%的财富集中在20%的人手中……80%的用户只使用20%的功能……20%的用户贡献了80%的访问量……)，这样用很小的长度就表示了整个ASCII范围的平均概率分布。比如下图，重排序后只用83个unique 的字符就表示了整个平均概率分布。</p><figure><img src="https://cdn.jsdelivr.net/gh/MarsAuthority/sec_pic@master/uPic/2023-02/frPmAf.jpg" alt="案例一" tabindex="0" loading="lazy"><figcaption>案例一</figcaption></figure><p>通过这种方法将已识别/确认的异常数据包做成签名(signature)，可以快速准确地检测其他地方可能出现的相同异常数据包。</p><h3 id="案例二" tabindex="-1"><a class="header-anchor" href="#案例二" aria-hidden="true">#</a> 案例二</h3><p>前面说到的方法是1-Gram的应用，然而1-Gram的简单性(平均字节概率分布)很容易受到拟态攻击(mimicry attacks)，攻击者可以通过填充无用字符的方法来伪造出正常的概率分布，从而绕过检测。于是他们又提出了基于N-gram N大于1 的方法。见《Anagram: A Content Anomaly Detector Resistant to Mimicry Attack》</p><p>N-gram的本质和1-Gram是一样的，只不过特征空间变大大，在计算的时间/内存开销也很大。比如一个TCP 数据包，长度是256，那么他的N-garm就有256^n。作者通过选取几个N值，比如3-Gram, 4-Gram, 5-Gram等等，然后用Bloom filter(原理相当于哈希表)进行存储。最后在ROC曲线中比较这些N-gram的召回率与准确率，选取合适的模型。如图：</p><figure><img src="https://cdn.jsdelivr.net/gh/MarsAuthority/sec_pic@master/uPic/2023-02/JsmuVT.jpg" alt="案例二" tabindex="0" loading="lazy"><figcaption>案例二</figcaption></figure><p>其他细节就略过，感兴趣可以自己查阅。</p><h2 id="其他应用" tabindex="-1"><a class="header-anchor" href="#其他应用" aria-hidden="true">#</a> 其他应用</h2><p>N-gram在安全领域还有很多其他的应用，比如HIDS(通过系统调用做异常检测)、恶意软件分类/识别、敏感词识别/屏蔽等等。但是效果却不好，误报、漏报严重。原因之前也提过，比如：测试还在用DARPA1999, KDD99等老样本、模型存在偏差样本性、缺乏实践等等问题。</p><p>开源项目参考：</p>`,27),P={href:"https://github.com/chwress/salad",target:"_blank",rel:"noopener noreferrer"},y=n("h2",{id:"参考资料",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#参考资料","aria-hidden":"true"},"#"),t(" 参考资料")],-1),N={href:"http://academiccommons.columbia.edu/catalog/ac%3A125704",target:"_blank",rel:"noopener noreferrer"},W={href:"http://ids.cs.columbia.edu/sites/default/files/anagram-camera-fixed.pdf",target:"_blank",rel:"noopener noreferrer"},w={href:"https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf",target:"_blank",rel:"noopener noreferrer"},x={href:"https://en.wikipedia.org/wiki/N-gram",target:"_blank",rel:"noopener noreferrer"};function G(A,X){const a=c("ExternalLinkIcon");return p(),i("div",null,[r,n("p",null,[t("I am working 后面很有可能出现"),l,t(", "),u,t(", "),h,t(" ，而不是"),k,t(", "),m,t(", "),g,t("。那么如何计算N-Grams呢？我们可以使用链式法则("),n("a",b,[t("Chain Rule"),s(a)]),t(")，求多个关联事件并存时的概率：")]),f,n("p",null,[t("下面用Bi-gram举个例子，语料库来自 "),n("a",_,[t("[Berkeley Restaurant Project]"),s(a)]),t(" ，总词数为 10132。")]),v,n("p",null,[n("a",P,[t("https://github.com/chwress/salad"),s(a)])]),y,n("ol",null,[n("li",null,[n("a",N,[t("Anomalous Payload-based Network Intrusion Detection"),s(a)])]),n("li",null,[n("a",W,[t("Anagram: A Content Anomaly Detector Resistant to Mimicry Attack"),s(a)])]),n("li",null,[n("a",w,[t("N-Grams tutorial"),s(a)])]),n("li",null,[n("a",x,[t("N-Grams wikipedia"),s(a)])])])])}const I=o(d,[["render",G],["__file","N-gram在安全领域的应用.html.vue"]]);export{I as default};
